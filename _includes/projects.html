<table class="projects">
  <tbody>
    <!-- ! Multimodal Perception for Autonomous Racing -->
    <!-- <tr><td>
      <div class="project_cell">
        <img src="assets/img/multimodal_perception.png" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
        </p>
        <p>
          [<a href="https://github.com/curious-ai/pytorch-td-rex/tree/offline" target="_blank">
            Code
          </a>]
          <br>
          Developed a lightweight UNet model for image segmentation with a Dice metric of 0.99 and streamlining data annotation using SAM. Enhanced sensor fusion by establishing ROS communication at 30 Hz for lidar and RGB cameras, and integrated the Dreamer algorithm with an Offline RL framework to achieve efficient online fine-tuning of the world model directly in the real world with a step ratio of less than 40%.
        </p>
      </div>
    </td></tr> -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/multimodal_perception_cover_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
          </p>
          <p>
            [<span style="color: grey; text-decoration: none;">Code (Coming Soon)</span>]
            [<a href="https://drive.google.com/file/d/1stXqYokZfWw-clVRVku7g8p80hlcKNYL/view?usp=sharing" target="_blank">Video</a>]
            <br>
            Developed a lightweight UNet model for image segmentation with a Dice metric of 0.99 and streamlining data annotation using SAM. Enhanced sensor fusion by establishing ROS communication at 30 Hz for lidar and RGB cameras, and integrated the Dreamer algorithm with an Offline RL framework to achieve efficient online fine-tuning of the world model directly in the real world with a step ratio of less than 40%.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! Wombat -->
    <!-- <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Electronic-waste Recycling</b></font>
        </p>
        <p>
          [<a href="https://github.com/SuryaPratapSingh37/KinoVision" target="_blank">
            Code
          </a>]
          <br>
          During a project collaboration with clients from Apple, I developed an object-agnostic gripping strategy to enhance e-waste manipulation speed and accuracy in a factory setup. Leading the simulation team, I integrated a novel 6-DoF parallel arm with a software framework, validating forward kinematics and locating singularities. I also implemented a DDPG + HER-based Deep RL algorithm, achieving a 95.55% success rate for pick-and-place operations at conveyor speeds up to 0.5 m/s.
        </p>
      </div>
    </td></tr> -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/wombat_results_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Electronic-waste Recycling</b></font>
          </p>
          <p>
            [<a href="https://github.com/biorobotics/Wombat_robosuite" target="_blank">Code</a>]
            [<a href="https://docs.google.com/presentation/d/1uCb-5oLUr9LShpgxhDzhjrvFSEvU9o_i/edit?usp=sharing&ouid=105419178815487827350&rtpof=true&sd=true" target="_blank">Slides</a>]
            <br>
            During a project collaboration with clients from Apple, I developed an object-agnostic gripping strategy to enhance e-waste manipulation speed and accuracy in a factory setup. Leading the simulation team, I integrated a novel 6-DoF parallel arm with a software framework, validating forward kinematics and locating singularities. I also implemented a DDPG + HER-based Deep RL algorithm, achieving a 95.55% success rate for pick-and-place operations at conveyor speeds up to 0.5 m/s.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! ROB 550 MBot-->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/mbot_slam_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Robot-Navigation-SLAM</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/1_I9DzmosTZh322cdZhQ5kC2oYU9Sklzp/view?usp=drive_link" target="_blank">PDF</a>]
            <br>
            The project aims to develop a comprehensive robotic system capable of autonomous navigation. This includes designing a feedback controller to regulate motor speed and a velocity-based movement controller, implementing 2D mapping using Lidar for simultaneous localization and mapping (SLAM), and constructing a path planner for effective navigation within the mapped environment.
          </p>
        </div>
      </td>
    </tr>
    
    <!-- ! ROB550 Armlab -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/armlab_block_sorting_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>KinoVision</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            [<a href="https://drive.google.com/file/d/19UP8UG3t_4UYOeVGo8vzhoAtccAKI-aY/view?usp=sharing" target="_blank">PDF</a>]
            <br>
            This is a comprehensive project aimed at developing computer vision algorithms integrated with the kinematics of a 5-DOF manipulator. The project focuses on detecting and manipulating blocks within the reach of the robot arm.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! Autonomous UAV -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/UAV_view.png" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Autonomous UAV-based Search and Rescue</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/UAV-SaR-Tracking" target="_blank">Code</a>]
            <br>
            ROS Package to implement reinforcement learning algorithms for autonomous navigation of UAV in a Search-and-Rescue environment. A PID algorithm is employed for position control.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! OriCon3D -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/oricon3d_gif.gif" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>OriCon3D: Monocular 3D Object Detection</b></font>
          </p>
          <p>
            [<a href="https://github.com/DhyeyR-007/OriCon3D/tree/main" target="_blank">Code</a>]
            <br>
            A Robust, light-weight and unique 3D object detection architecture providing results (better than the conventional architectures) in real-time autonomous driving scenarios
          </p>
        </div>
      </td>
    </tr>
    <!-- ! Twilight-SLAM -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/twilight_slam.png" width="250" height="200" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Twilight SLAM: Navigating Low-Light Environments</b></font>
          </p>
          <p>
            [<a href="https://github.com/TwilightSLAM" target="_blank">Code</a>]
            <br>
            Developed a Deep Learning-based image enhancement architecture to brighten low-light images, significantly improving feature extraction. Additionally, I integrated these enhancement modules with ORB-SLAM3 and SuperPoint-SLAM frameworks, resulting in validated improvements in localization accuracy under low-light conditions.
          </p>
        </div>
      </td>
    </tr>
  </tbody>
</table>
