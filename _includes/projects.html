<table class="projects">
  <tbody>
    <!-- ! Multimodal Perception for Autonomous Racing -->
    <!-- <tr><td>
      <div class="project_cell">
        <img src="assets/img/multimodal_perception.png" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
        </p>
        <p>
          [<a href="https://github.com/curious-ai/pytorch-td-rex/tree/offline" target="_blank">
            Code
          </a>]
          <br>
          Developed a lightweight UNet model for image segmentation with a Dice metric of 0.99 and streamlining data annotation using SAM. Enhanced sensor fusion by establishing ROS communication at 30 Hz for lidar and RGB cameras, and integrated the Dreamer algorithm with an Offline RL framework to achieve efficient online fine-tuning of the world model directly in the real world with a step ratio of less than 40%.
        </p>
      </div>
    </td></tr> -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/multimodal_perception.png" width="250" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Multimodal Perception for Autonomous Racing</b></font>
          </p>
          <p>
            [<span style="color: grey; text-decoration: none;">Code (Coming Soon)</span>]
            <br>
            Developed a lightweight UNet model for image segmentation with a Dice metric of 0.99 and streamlining data annotation using SAM. Enhanced sensor fusion by establishing ROS communication at 30 Hz for lidar and RGB cameras, and integrated the Dreamer algorithm with an Offline RL framework to achieve efficient online fine-tuning of the world model directly in the real world with a step ratio of less than 40%.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! Wombat -->
    <!-- <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Electronic-waste Recycling</b></font>
        </p>
        <p>
          [<a href="https://github.com/SuryaPratapSingh37/KinoVision" target="_blank">
            Code
          </a>]
          <br>
          During a project collaboration with clients from Apple, I developed an object-agnostic gripping strategy to enhance e-waste manipulation speed and accuracy in a factory setup. Leading the simulation team, I integrated a novel 6-DoF parallel arm with a software framework, validating forward kinematics and locating singularities. I also implemented a DDPG + HER-based Deep RL algorithm, achieving a 95.55% success rate for pick-and-place operations at conveyor speeds up to 0.5 m/s.
        </p>
      </div>
    </td></tr> -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/wombat_results_gif.gif" width="250" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Electronic-waste Recycling</b></font>
          </p>
          <p>
            [<a href="https://github.com/biorobotics/Wombat_robosuite" target="_blank">Code</a>]
            <br>
            During a project collaboration with clients from Apple, I developed an object-agnostic gripping strategy to enhance e-waste manipulation speed and accuracy in a factory setup. Leading the simulation team, I integrated a novel 6-DoF parallel arm with a software framework, validating forward kinematics and locating singularities. I also implemented a DDPG + HER-based Deep RL algorithm, achieving a 95.55% success rate for pick-and-place operations at conveyor speeds up to 0.5 m/s.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! ROB 550 MBot-->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/mbot_slam_gif.gif" width="250" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>Robot-Navigation-SLAM</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            <br>
            The project aims to develop a comprehensive robotic system capable of autonomous navigation. This includes designing a feedback controller to regulate motor speed and a velocity-based movement controller, implementing 2D mapping using Lidar for simultaneous localization and mapping (SLAM), and constructing a path planner for effective navigation within the mapped environment.
          </p>
        </div>
      </td>
    </tr>
    
    <!-- ! ROB550 Armlab -->
    <tr>
      <td>
        <div class="project_cell">
          <img src="assets/img/armlab_block_sorting_gif.gif" width="250" class="zoom">
        </div>
        <div class="project_cell">
          <p style="font-size:18px;margin-bottom: 0px;">
            <font color="black"><b>KinoVision</b></font>
          </p>
          <p>
            [<a href="https://github.com/SuryaPratapSingh37/Robot-Navigation-SLAM" target="_blank">Code</a>]
            <br>
            This is a comprehensive project aimed at developing computer vision algorithms integrated with the kinematics of a 5-DOF manipulator. The project focuses on detecting and manipulating blocks within the reach of the robot arm.
          </p>
        </div>
      </td>
    </tr>
    <!-- ! Autonomous UAV -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Single-Image to Camera Pose with iNeRF and PoseCNN</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/fast-inerf?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/fast-inerf" target="_blank">
            Code
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            PDF
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            System Diagram
          </a>]
          <br>
          We present an efficient and robust system for view synthesis and pose estimation by integrating PoseCNN and iNeRF. Our method leverages the pose and object segmentation predictions from PoseCNN to improve the initial camera pose estimation and accelerate the optimization process in iNeRF.
        </p>
      </div>
    </td></tr>
  </tbody>
</table>
